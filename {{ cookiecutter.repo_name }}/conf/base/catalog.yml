# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/data/data_catalog.html
#
# An explanation of Kedro's data layer philosophy can be found here:
# https://towardsdatascience.com/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71

source_data:
  type: pandas.CSVDataSet
  filepath: https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv
  layer: raw

raw_labels:
  type: pandas.ParquetDataSet
  filepath: ${storage_prefix}data/02_intermediate/labels.pq
  credentials: abs_creds # See hooks.yml
  layer: intermediate

raw_features:
  type: pandas.ParquetDataSet
  filepath: ${storage_prefix}data/02_intermediate/raw_features.pq
  credentials: abs_creds # See hooks.yml
  layer: intermediate

features:
  type: pandas.ParquetDataSet
  filepath: ${storage_prefix}data/05_model_input/features.pq
  credentials: abs_creds # See hooks.yml
  layer: intermediate

X_train:
  type: MemoryDataSet
  layer: model_input

y_train:
  type: MemoryDataSet
  layer: model_input

X_test:
  type: MemoryDataSet
  layer: model_input

y_test:
  type: MemoryDataSet
  layer: model_input

trained_model:
  type: pickle.PickleDataSet
  filepath: data/06_models/trained_model.pkl # artifacts should be local for kedro-mlflow
  layer: models

accuracy:
  type: kedro_mlflow.io.metrics.MlflowMetricDataSet

predictions:
  type: pandas.ParquetDataSet
  filepath: ${storage_prefix}data/07_model_output/predictions.pq
  credentials: abs_creds # See hooks.yml
  layer: model_output